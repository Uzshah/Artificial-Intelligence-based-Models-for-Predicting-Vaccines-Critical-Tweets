{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bertweet Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x1457063512501522436</td>\n",
       "      <td>I get the neg sentiment towards pharma, but re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1093905117940760577</td>\n",
       "      <td>mr o’brien you know i love and respect you but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x1423285108551806976</td>\n",
       "      <td>Get ready California. You're next.\\nThere is n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x1449650082173923330</td>\n",
       "      <td>So I don’t have Covid, but I do have this cold...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x1444610070684311557</td>\n",
       "      <td>California becomes the first US state to requi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>x1437534009349771268</td>\n",
       "      <td>Is it safe to get a COVID vaccine and a flu sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>x1429879445607927810</td>\n",
       "      <td>Why is Pfizer 1st for approval when they have ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>x1430970906772328451</td>\n",
       "      <td>Before you get the jab, please watch this. #va...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>x1438302913689006084</td>\n",
       "      <td>Moderna on Wednesday posted a study suggesting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>x319489006654091264</td>\n",
       "      <td>New post! An open letter to my dad, on the occ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IDs                                               text  \\\n",
       "0    x1457063512501522436  I get the neg sentiment towards pharma, but re...   \n",
       "1    x1093905117940760577  mr o’brien you know i love and respect you but...   \n",
       "2    x1423285108551806976  Get ready California. You're next.\\nThere is n...   \n",
       "3    x1449650082173923330  So I don’t have Covid, but I do have this cold...   \n",
       "4    x1444610070684311557  California becomes the first US state to requi...   \n",
       "..                    ...                                                ...   \n",
       "795  x1437534009349771268  Is it safe to get a COVID vaccine and a flu sh...   \n",
       "796  x1429879445607927810  Why is Pfizer 1st for approval when they have ...   \n",
       "797  x1430970906772328451  Before you get the jab, please watch this. #va...   \n",
       "798  x1438302913689006084  Moderna on Wednesday posted a study suggesting...   \n",
       "799   x319489006654091264  New post! An open letter to my dad, on the occ...   \n",
       "\n",
       "     label  \n",
       "0        0  \n",
       "1        1  \n",
       "2        1  \n",
       "3        0  \n",
       "4        1  \n",
       "..     ...  \n",
       "795      0  \n",
       "796      1  \n",
       "797      1  \n",
       "798      0  \n",
       "799      0  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Please download tweets using Twitter API using the IDs of the tweets in Manualy Label Tweets.xlsx file\n",
    "df = pd.read_excel(\"Manualy Label Tweets.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>text</th>\n",
       "      <th>textm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>394</td>\n",
       "      <td>394</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IDs  text  textm\n",
       "label                  \n",
       "0      406   406    406\n",
       "1      394   394    394"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"label\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[\"text\"].tolist()\n",
    "y = df[\"label\"].tolist()\n",
    "import numpy as np\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.48\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:27 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.16\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.09\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:27 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.50\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.33\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.08\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:27 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from LM import DLClassifier\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "bertweet = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    train_df, test_df = df.iloc[train_index], df.iloc[test_index]\n",
    "    dlc = DLClassifier(mp = \"vinai/bertweet-base\", num_classes=2)\n",
    "    dlc.prepare_dataloaders(train_df, test_df)\n",
    "    \n",
    "    epochs = 5\n",
    "    total_steps = len(dlc.train_dataloader) * epochs\n",
    "    dlc.prepare_model(total_steps = total_steps)\n",
    "    dlc.train(epochs)\n",
    "    \n",
    "    result = dlc.predict(dlc.test_dataloader)\n",
    "    pred_labels = np.argmax(result, axis = 1)\n",
    "    pred_scores = softmax(result, axis=1)[:, 1]\n",
    "    y_true = test_df[\"label\"].tolist()\n",
    "    cm = confusion_matrix(y_true, pred_labels)\n",
    "    bertweet.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.859375 \t 0.865 \t 0.8823529411764706 \t 0.8375634517766497\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(bertweet, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.49\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.11\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.00\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:26 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.45\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.15\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.02\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:26 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.46\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.27\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.13\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of     67.    Elapsed: 0:00:03.\n",
      "\n",
      "  Average training loss: 0.01\n",
      "  Training epoch took: 0:00:05\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:26 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from LM import DLClassifier\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "bert = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    train_df, test_df = df.iloc[train_index], df.iloc[test_index]\n",
    "    dlc = DLClassifier(mp = \"bert-base-uncased\", num_classes=2)\n",
    "    dlc.prepare_dataloaders(train_df, test_df)\n",
    "    \n",
    "    epochs = 5\n",
    "    total_steps = len(dlc.train_dataloader) * epochs\n",
    "    dlc.prepare_model(total_steps = total_steps)\n",
    "#     dlc.model.tokenizer.pad_token = dlc.model.tokenizer.eos_token\n",
    "    dlc.train(epochs)\n",
    "    \n",
    "    result = dlc.predict(dlc.test_dataloader)\n",
    "    pred_labels = np.argmax(result, axis = 1)\n",
    "    pred_scores = softmax(result, axis=1)[:, 1]\n",
    "    y_true = test_df[\"label\"].tolist()\n",
    "    cm = confusion_matrix(y_true, pred_labels)\n",
    "    bert.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8507853403141361 \t 0.8575 \t 0.8783783783783784 \t 0.8248730964467006\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(bert, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets Cleaning for classical AI-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"textm\"] = df[\"text\"].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "df[\"textm\"] = df[\"textm\"].apply(lambda x: re.sub(r\"https\\S+\", \"\", x))\n",
    "df[\"textm\"] = df[\"textm\"].apply(lambda x: re.sub(r\"@\\S+\", \"\", x))\n",
    "df[\"textm\"] = df[\"textm\"].apply(lambda x: re.sub(r\"#\\S+\", \"\", x))\n",
    "df[\"textm\"] = df[\"textm\"].apply(lambda x: remove_emojis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "X = df[\"textm\"].tolist()\n",
    "y = df[\"label\"].tolist()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical AI-based models experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=4000, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "rf = []\n",
    "y = np.array(y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    rf.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7024901703800787 \t 0.7162499999999999 \t 0.7262872628726287 \t 0.6802030456852792\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(rf, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "lr = []\n",
    "y = np.array(y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier = LogisticRegression(random_state=0)\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    lr.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6990801576872536 \t 0.71375 \t 0.7247956403269754 \t 0.6751269035532995\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(lr, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "svc = []\n",
    "y = np.array(y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier = LinearSVC()\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    svc.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6926070038910506 \t 0.70375 \t 0.7082228116710876 \t 0.6776649746192893\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(svc, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "nb = []\n",
    "y = np.array(y)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, y_train) \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    nb.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7032085561497327 \t 0.7225 \t 0.7429378531073447 \t 0.6675126903553299\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = np.mean(nb, axis=0).ravel()\n",
    "accuracy = (tp + tn)/(tn+fp+fn+tp) \n",
    "precision = tp/(tp +fp)\n",
    "recall = tp/(tp + fn)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "print(f1,\"\\t\", accuracy,\"\\t\", precision,\"\\t\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x1416702894041075715</td>\n",
       "      <td>I had no covid symptoms since covid madness ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x1440062609974120456</td>\n",
       "      <td>How insulting to all those tradies &amp;amp; const...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x1416734488030285827</td>\n",
       "      <td>So why get a vax a couple of weeks ago?no cred...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x1423737716932497408</td>\n",
       "      <td>Russia introduced a strict vax passports compl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x1456459724422066191</td>\n",
       "      <td>@masterleolee @joerogan Check this out Rogan. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>x1455877121818103808</td>\n",
       "      <td>Fellow NYC parents: Walgreens has opened vax a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>x1449026776336846848</td>\n",
       "      <td>Judge stops father from seeing daughter unless...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>x1417375476419604506</td>\n",
       "      <td>So... this guy has the jab, then contracts cov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>x1423619136018698244</td>\n",
       "      <td>Holy Covid!  There's an interview on The Daily...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>x1430970906772328451</td>\n",
       "      <td>Before you get the jab, please watch this. #va...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      IDs                                               text  \\\n",
       "0    x1416702894041075715  I had no covid symptoms since covid madness ar...   \n",
       "1    x1440062609974120456  How insulting to all those tradies &amp; const...   \n",
       "2    x1416734488030285827  So why get a vax a couple of weeks ago?no cred...   \n",
       "3    x1423737716932497408  Russia introduced a strict vax passports compl...   \n",
       "4    x1456459724422066191  @masterleolee @joerogan Check this out Rogan. ...   \n",
       "..                    ...                                                ...   \n",
       "795  x1455877121818103808  Fellow NYC parents: Walgreens has opened vax a...   \n",
       "796  x1449026776336846848  Judge stops father from seeing daughter unless...   \n",
       "797  x1417375476419604506  So... this guy has the jab, then contracts cov...   \n",
       "798  x1423619136018698244  Holy Covid!  There's an interview on The Daily...   \n",
       "799  x1430970906772328451  Before you get the jab, please watch this. #va...   \n",
       "\n",
       "     label  \n",
       "0        1  \n",
       "1        0  \n",
       "2        0  \n",
       "3        1  \n",
       "4        1  \n",
       "..     ...  \n",
       "795      0  \n",
       "796      0  \n",
       "797      1  \n",
       "798      0  \n",
       "799      1  \n",
       "\n",
       "[800 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"IDs\",\"text\",\"label\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df[\"text\"].tolist()\n",
    "y = df[\"label\"].tolist()\n",
    "import numpy as np\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"sample_tweets.csv\")\n",
    "test_df = test_df[[\"IDs\",\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2221: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    100.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    100.    Elapsed: 0:00:06.\n",
      "\n",
      "  Average training loss: 0.44\n",
      "  Training epoch took: 0:00:08\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    100.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    100.    Elapsed: 0:00:06.\n",
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epoch took: 0:00:08\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    100.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    100.    Elapsed: 0:00:06.\n",
      "\n",
      "  Average training loss: 0.21\n",
      "  Training epoch took: 0:00:08\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    100.    Elapsed: 0:00:06.\n",
      "  Batch    80  of    100.    Elapsed: 0:00:09.\n",
      "\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:00:10\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch    40  of    100.    Elapsed: 0:00:03.\n",
      "  Batch    80  of    100.    Elapsed: 0:00:06.\n",
      "\n",
      "  Average training loss: 0.03\n",
      "  Training epoch took: 0:00:08\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:42 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "dlc = DLClassifier(mp = \"vinai/bertweet-base\", num_classes=2)\n",
    "dlc.prepare_dataloaders(df, test_df)\n",
    "\n",
    "epochs = 5\n",
    "total_steps = len(dlc.train_dataloader) * epochs\n",
    "dlc.prepare_model(total_steps = total_steps)\n",
    "dlc.train(epochs)\n",
    "\n",
    "result = dlc.predict(dlc.test_dataloader)\n",
    "pred_labels = np.argmax(result, axis = 1)\n",
    "pred_scores = softmax(result, axis=1)[:, 1]\n",
    "test_df[\"label\"] = pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDs</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5914</td>\n",
       "      <td>5914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7133</td>\n",
       "      <td>7133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        IDs  text\n",
       "label            \n",
       "0      5914  5914\n",
       "1      7133  7133"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby([\"label\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_excel(\"Sample_label_Tweets.xlsx\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
